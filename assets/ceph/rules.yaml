groups:
- name: ceph.rules
  rules:
  - expr: |
      kube_node_status_condition{condition="Ready",job="kube-state-metrics",status="true"} * on (node) group_right() max(label_replace(ceph_disk_occupation{job="rook-ceph-mgr"},"node","$1","exported_instance","(.*)")) by (node)
    record: cluster:ceph_node_down:join_kube
  - expr: |
      avg(max by(instance) (label_replace(label_replace(ceph_disk_occupation{job="rook-ceph-mgr"}, "instance", "$1", "exported_instance", "(.*)"), "device", "$1", "device", "/dev/(.*)") * on(instance, device) group_right() (irate(node_disk_read_time_seconds_total[1m]) + irate(node_disk_write_time_seconds_total[1m]) / (clamp_min(irate(node_disk_reads_completed_total[1m]), 1) + irate(node_disk_writes_completed_total[1m])))))
    record: cluster:ceph_disk_latency:join_ceph_node_disk_irate1m
- name: telemeter.rules
  rules:
  - expr: |
      count(ceph_osd_metadata{job="rook-ceph-mgr"})
    record: job:ceph_osd_metadata:count
  - expr: |
      count(kube_persistentvolume_info)
    record: job:kube_pv:count
  - expr: |
      sum(ceph_pool_rd{job="rook-ceph-mgr"}+ ceph_pool_wr{job="rook-ceph-mgr"})
    record: job:ceph_pools_iops:total
  - expr: |
      sum(ceph_pool_rd_bytes{job="rook-ceph-mgr"}+ ceph_pool_wr_bytes{job="rook-ceph-mgr"})
    record: job:ceph_pools_iops_bytes:total
  - expr: |
      count(count(ceph_mon_metadata{job="rook-ceph-mgr"} or ceph_osd_metadata{job="rook-ceph-mgr"} or ceph_rgw_metadata{job="rook-ceph-mgr"} or ceph_mds_metadata{job="rook-ceph-mgr"} or ceph_mgr_metadata{job="rook-ceph-mgr"}) by(ceph_version))
    record: job:ceph_versions_running:count
